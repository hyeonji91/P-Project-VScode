{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import modules.holistic_module as hm\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from modules.utils import Vector_Normalization\n",
    "from modules.utils import Merge_Jamo_With_LLM\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "import torch\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742823176.145226 6107532 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Max\n"
     ]
    }
   ],
   "source": [
    "#설정\n",
    "fontpath = \"AppleGothic.ttf\"\n",
    "font = ImageFont.truetype(fontpath, 40)\n",
    "\n",
    "actions = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',\n",
    "             'ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ',\n",
    "             'ㅐ', 'ㅒ', 'ㅔ', 'ㅖ', 'ㅢ', 'ㅚ', 'ㅟ']\n",
    "seq_length = 10\n",
    "\n",
    "# MediaPipe holistic model\n",
    "detector = hm.HolisticDetector(min_detection_confidence=0.3)\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"multi_hand_gesture_classifier.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.10s/it]\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x3aad650e0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m MllamaProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBllossom/llama-3.2-Korean-Bllossom-AICA-5B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://cdn.discordapp.com/attachments/1156141391798345742/1313407928287494164/E18489E185B3E1848FE185B3E18485E185B5E186ABE18489E185A3E186BA202021-11-1620E1848BE185A9E18492E185AE2011.png?ex=675005f4&is=674eb474&hm=fc9c4231203f53c27f6edd2420961c182dd4a1ed14d4b73e04127f11393729af&\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p-project/lib/python3.9/site-packages/PIL/Image.py:3532\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3530\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3531\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3532\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x3aad650e0>"
     ]
    }
   ],
   "source": [
    "# korean bllossom model\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    \"Bllossom/llama-3.2-Korean-Bllossom-AICA-5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto')\n",
    "processor = MllamaProcessor.from_pretrained(\"Bllossom/llama-3.2-Korean-Bllossom-AICA-5B\")\n",
    "url = \"https://cdn.discordapp.com/attachments/1156141391798345742/1313407928287494164/E18489E185B3E1848FE185B3E18485E185B5E186ABE18489E185A3E186BA202021-11-1620E1848BE185A9E18492E185AE2011.png?ex=675005f4&is=674eb474&hm=fc9c4231203f53c27f6edd2420961c182dd4a1ed14d4b73e04127f11393729af&\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 17:23:51.861 Python[61882:2287504] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "seq = []\n",
    "action_seq = []\n",
    "last_action = None\n",
    "zamo_list = [\"\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742199833.702270 2291555 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-03-17 17:23:54.098 Python[61882:2287504] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-17 17:23:54.098 Python[61882:2287504] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = detector.findHolistic(img, draw=True)\n",
    "    _, right_hand_lmList = detector.findRighthandLandmark(img)\n",
    "\n",
    "    # 오른손이 감지되었을 때\n",
    "    if right_hand_lmList is not None:\n",
    "\n",
    "        joint = np.zeros((42, 2))\n",
    "        \n",
    "        # 오른손 랜드마크 리스트\n",
    "        for j, lm in enumerate(right_hand_lmList.landmark):\n",
    "            joint[j] = [lm.x, lm.y]\n",
    "\n",
    "        # 벡터 정규화 : 백터, 앵글\n",
    "        vector, angle_label = Vector_Normalization(joint)\n",
    "    \n",
    "        # 백터와 각도를 하나의 배열로 합침\n",
    "        d = np.concatenate([vector.flatten(), angle_label.flatten()])\n",
    "        \n",
    "\n",
    "        seq.append(d)\n",
    "        if len(seq) < seq_length: # 만약 시퀀스의 길이가 10보다 작다면 다음으로 넘어감\n",
    "            continue\n",
    "        \n",
    "        # 시퀀스 데이터와 넘파이화\n",
    "        # 시퀀스 데이터를 최근 10개로 자름, 입력으로 쓰기 위해 차원 확장\n",
    "        input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0) \n",
    "        input_data = np.array(input_data, dtype=np.float32)\n",
    "\n",
    "        # tflite 모델을 활용한 예측\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke() # 모델 실행\n",
    "        y_pred = interpreter.get_tensor(output_details[0]['index']) # 예측 결과 가져오기\n",
    "        i_pred = int(np.argmax(y_pred[0])) # 가장 확률이 높은 클래스\n",
    "        conf = y_pred[0][i_pred] # 예측 결과의 확률 (신뢰도)\n",
    "\n",
    "        if conf < 0.9: # 신뢰도가 90% 미만이면 무시\n",
    "            continue\n",
    "\n",
    "        action = actions[i_pred] # 라벨 -> 실제값\n",
    "        action_seq.append(action) # 시퀀스에 추가\n",
    "\n",
    "        if len(action_seq) < 3: # 시퀀스 길이가 3보다 작으면 다음 루프로 넘어감\n",
    "            continue\n",
    "\n",
    "        this_action = '?'\n",
    "        if action_seq[-1] == action_seq[-2] == action_seq[-3]: # 연속 3번 같으면 현재 액션으로 확정\n",
    "            this_action = action\n",
    "\n",
    "            # if last_action != this_action: # 이전 액션과 현재 액션이 다르면 \n",
    "            #     last_action = this_action\n",
    "        \n",
    "        # 기록된 한글 파악\n",
    "        if zamo_list[-1] != this_action: # 만약 전에 기록된 글자와 이번 글자가 다르다면\n",
    "            zamo_list.append(this_action)\n",
    "            print(\"zamo list : \",zamo_list)\n",
    "        zamo_str = ''.join(zamo_list) # 리스트에 있는 단어 합침\n",
    "        unitl_action = Merge_Jamo_With_LLM(zamo_str) # 합친 단어 한글로 만들기\n",
    "        \n",
    "        \n",
    "        # 한글 폰트 출력    \n",
    "        img_pil = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "        draw.text((10, 30), f'{unitl_action.upper()}', font=font, fill=(255, 255, 255))\n",
    "\n",
    "        img = np.array(img_pil)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
