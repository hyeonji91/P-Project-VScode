{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import modules.holistic_module as hm\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from modules.utils import Vector_Normalization\n",
    "from PIL import ImageFont, ImageDraw, Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742199829.737184 2287504 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Max\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "/Users/hyeonji/Library/Python/3.9/lib/python/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742199829.807246 2291551 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.817706 2291549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.820072 2291549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.820079 2291552 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.820336 2291559 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.824429 2291559 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.826151 2291552 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742199829.826325 2291549 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "#설정\n",
    "fontpath = \"AppleGothic.ttf\"\n",
    "font = ImageFont.truetype(fontpath, 40)\n",
    "\n",
    "actions = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',\n",
    "             'ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ',\n",
    "             'ㅐ', 'ㅒ', 'ㅔ', 'ㅖ', 'ㅢ', 'ㅚ', 'ㅟ']\n",
    "seq_length = 10\n",
    "\n",
    "# MediaPipe holistic model\n",
    "detector = hm.HolisticDetector(min_detection_confidence=0.3)\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"multi_hand_gesture_classifier.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 17:23:51.861 Python[61882:2287504] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "seq = []\n",
    "action_seq = []\n",
    "last_action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742199833.702270 2291555 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-03-17 17:23:54.098 Python[61882:2287504] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-17 17:23:54.098 Python[61882:2287504] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = detector.findHolistic(img, draw=True)\n",
    "    # _, left_hand_lmList = detector.findLefthandLandmark(img)\n",
    "    _, right_hand_lmList = detector.findRighthandLandmark(img)\n",
    "\n",
    "    # if left_hand_lmList is not None and right_hand_lmList is not None:\n",
    "    if right_hand_lmList is not None:\n",
    "\n",
    "        joint = np.zeros((42, 2))\n",
    "        # 왼손 랜드마크 리스트\n",
    "        # for j, lm in enumerate(left_hand_lmList.landmark):\n",
    "            # joint[j] = [lm.x, lm.y]\n",
    "        \n",
    "        # 오른손 랜드마크 리스트\n",
    "        for j, lm in enumerate(right_hand_lmList.landmark):\n",
    "            # joint[j+21] = [lm.x, lm.y]\n",
    "            joint[j] = [lm.x, lm.y]\n",
    "\n",
    "        # 좌표 정규화\n",
    "        # full_scale = Coordinate_Normalization(joint)\n",
    "\n",
    "        # 벡터 정규화\n",
    "        vector, angle_label = Vector_Normalization(joint)\n",
    "\n",
    "        # 위치 종속성을 가지는 데이터 저장\n",
    "        # d = np.concatenate([joint.flatten(), angle_label])\n",
    "    \n",
    "        # 벡터 정규화를 활용한 위치 종속성 제거\n",
    "        d = np.concatenate([vector.flatten(), angle_label.flatten()])\n",
    "\n",
    "        # 정규화 좌표를 활용한 위치 종속성 제거 \n",
    "        # d = np.concatenate([full_scale, angle_label.flatten()])\n",
    "        \n",
    "\n",
    "        seq.append(d)\n",
    "\n",
    "        if len(seq) < seq_length:\n",
    "            continue\n",
    "\n",
    "        # Test model on random input data.\n",
    "        # input_shape = input_details[0]['shape']\n",
    "        # input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "        \n",
    "        # 시퀀스 데이터와 넘파이화\n",
    "        input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "        input_data = np.array(input_data, dtype=np.float32)\n",
    "\n",
    "        # tflite 모델을 활용한 예측\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        y_pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "        i_pred = int(np.argmax(y_pred[0]))\n",
    "        conf = y_pred[0][i_pred]\n",
    "\n",
    "        if conf < 0.9:\n",
    "            continue\n",
    "\n",
    "        action = actions[i_pred]\n",
    "        action_seq.append(action)\n",
    "\n",
    "        if len(action_seq) < 3:\n",
    "            continue\n",
    "\n",
    "        this_action = '?'\n",
    "        if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "            this_action = action\n",
    "\n",
    "            if last_action != this_action:\n",
    "                last_action = this_action\n",
    "        \n",
    "        # 기록된 한글 파악\n",
    "        if zamo_list[-1] != this_action: # 만약 전에 기록된 글자와 이번 글자가 다르다면\n",
    "            zamo_list.append(this_action)\n",
    "        \n",
    "        zamo_str = ''.join(zamo_list) # 리스트에 있는 단어 합침\n",
    "        unitl_action = join_jamos(zamo_str) # 합친 단어 한글로 만들기\n",
    "        \n",
    "        \n",
    "        # 한글 폰트 출력    \n",
    "        img_pil = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(img_pil)\n",
    "        '''\n",
    "        draw.text((int(right_hand_lmList.landmark[0].x * img.shape[1]), int(right_hand_lmList.landmark[0].y * img.shape[0] + 20)),\n",
    "                  f'{this_action.upper()}', \n",
    "                  font=font, \n",
    "                  fill=(255, 255, 255))\n",
    "        '''\n",
    "        draw.text((10, 30), f'{action.upper()}', font=font, fill=(255, 255, 255))\n",
    "\n",
    "        img = np.array(img_pil)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # cv2.putText(img, f'{this_action.upper()}', org=(int(right_hand_lmList.landmark[0].x * img.shape[1]), int(right_hand_lmList.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
