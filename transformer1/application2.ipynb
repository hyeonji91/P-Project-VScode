{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeonji/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from PIL import ImageFont, ImageDraw, Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : mediapipe 손 keypoint (21,4)\n",
    "# out : angle (15,)\n",
    "def cal_angle(joint):\n",
    "    # 벡터 계산\n",
    "    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "    v = v2 - v1 # [20, 3]\n",
    "\n",
    "    # normalize v : 길이로 나누기\n",
    "    v = v/np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # arccos dot product로 앵글 구하기\n",
    "    angle = np.arccos(np.einsum('nt, nt->n',\n",
    "                                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18], :],\n",
    "                                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "    # radian을 degree(도)로 변경\n",
    "    angle = np.degrees(angle) \n",
    "\n",
    "    angle = np.array([angle], dtype=np.float32)\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_angle(results):\n",
    "\n",
    "    # 왼손 키포인트 추출\n",
    "    if results.left_hand_landmarks:\n",
    "        joint_left = np.zeros((21,4))\n",
    "        # 키포인트 추출\n",
    "        for j, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            joint_left[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "        # 앵글 계산 [15,]\n",
    "        angle_left = cal_angle(joint_left)\n",
    "    else:\n",
    "        joint_left = np.zeros((21,4))\n",
    "        angle_left = [np.zeros((15,))]\n",
    "\n",
    "    # 오른손 키포인트 추출\n",
    "    if results.right_hand_landmarks:\n",
    "        joint_right = np.zeros((21,4))\n",
    "        # 키포인트 추출\n",
    "        for j, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            joint_right[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "        # 앵글 계산\n",
    "        angle_right = cal_angle(joint_right)\n",
    "    else:\n",
    "        joint_right = np.zeros((21,4))\n",
    "        angle_right = [np.zeros((15,))]\n",
    "    \n",
    "\n",
    "    frame_angle = np.concatenate([angle_left, angle_right]).flatten()\n",
    "\n",
    "    return frame_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 from train_transformer.ipynb\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_angles, num_classes, seq_len=60, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(num_angles, d_model)# 각 프레임의 앵글 값을 d_model 차원으로 변환\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, seq_len, d_model)) # 위치 인코딩\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout) \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_encoder # B, T, d_model\n",
    "        x = self.transformer_encoder(x) # B,T, d_model\n",
    "        x = x.mean(dim=1) # 전체 시퀀스에 대한 평균 (B, d_model)\n",
    "        return self.fc(x)  # (B, num_classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '10': 2, '100': 3, '1000': 4, '10000': 5, '11': 6, '112': 7, '119': 8, '12': 9, '13': 10, '14': 11, '15': 12, '16': 13, '17': 14, '18': 15, '19': 16, '2': 17, '20': 18, '21': 19, '22': 20, '23': 21, '24': 22, '25': 23, '26': 24, '27': 25, '28': 26, '29': 27, '3': 28, '30': 29, '31': 30, '32': 31, '33': 32, '34': 33, '35': 34, '36': 35, '37': 36, '38': 37, '39': 38, '4': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '5': 50, '50': 51, '51': 52, '52': 53, '53': 54, '54': 55, '55': 56, '56': 57, '57': 58, '58': 59, '59': 60, '6': 61, '60': 62, '61': 63, '62': 64, '63': 65, '64': 66, '65': 67, '66': 68, '67': 69, '68': 70, '69': 71, '7': 72, '70': 73, '71': 74, '72': 75, '73': 76, '74': 77, '75': 78, '76': 79, '77': 80, '78': 81, '79': 82, '8': 83, '80': 84, '81': 85, '82': 86, '83': 87, '84': 88, '85': 89, '86': 90, '87': 91, '88': 92, '89': 93, '9': 94, '90': 95, '91': 96, '92': 97, '93': 98, '94': 99, '95': 100, '96': 101, '97': 102, '98': 103, '99': 104, '가렵다': 105, '가스': 106, '가슴': 107, '가시': 108, '각목': 109, '갇히다': 110, '감금': 111, '감전': 112, '강': 113, '강남구': 114, '강동구': 115, '강북구': 116, '강서구': 117, '강풍': 118, '개': 119, '거실': 120, '걸렸다': 121, '결박': 122, '경운기': 123, '경찰': 124, '경찰차': 125, '계곡': 126, '계단': 127, '고속도로': 128, '고압전선': 129, '고열': 130, '고장': 131, '골절': 132, '곰': 133, '공사장': 134, '공원': 135, '공장': 136, '관악구': 137, '광진구': 138, '교통사고': 139, '구급대': 140, '구급대원': 141, '구급차': 142, '구로구': 143, '구청': 144, '구해주세요': 145, '귀': 146, '금가다': 147, '금요일': 148, '금천구': 149, '급류': 150, '기절': 151, '기절하다': 152, '깔리다': 153, '끓는물': 154, '남자친구': 155, '남편': 156, '남학생': 157, '납치': 158, '낫': 159, '낯선남자': 160, '낯선사람': 161, '낯선여자': 162, '내년': 163, '내일': 164, '냄새나다': 165, '노원구': 166, '논': 167, '놀이터': 168, '농약': 169, '누나': 170, '누수': 171, '누전': 172, '누출': 173, '눈': 174, '다리': 175, '다음': 176, '달(월)': 177, '대문앞': 178, '도둑': 179, '도로': 180, '도봉구': 181, '독극물': 182, '독버섯': 183, '독사': 184, '동대문구': 185, '동생': 186, '동작구': 187, '동전': 188, '두드러기생기다': 189, '뒤': 190, '뒤통수': 191, '등': 192, '딸': 193, '떨어지다': 194, '뜨거운물': 195, '마당': 196, '마포구': 197, '말려주세요': 198, '말벌': 199, '맹견': 200, '머리': 201, '멧돼지': 202, '목': 203, '목요일': 204, '무너지다': 205, '무릎': 206, '문틈': 207, '물': 208, '밑에': 209, '바다': 210, '반점생기다': 211, '발': 212, '발가락': 213, '발목': 214, '발작': 215, '방망이': 216, '밭': 217, '배': 218, '배고프다': 219, '뱀': 220, '벌': 221, '범람': 222, '벼락': 223, '병원': 224, '보건소': 225, '보내주세요(경찰)': 226, '보내주세요(구급차)': 227, '복부': 228, '복통': 229, '볼': 230, '부러지다': 231, '부엌': 232, '불': 233, '불나다': 234, '붕괴': 235, '붕대': 236, '비닐하우스': 237, '비상약': 238, '빌라': 239, '뼈': 240, '사이': 241, '산': 242, '살충제': 243, '살해': 244, '삼키다': 245, '서대문구': 246, '서랍': 247, '서울시': 248, '서초구': 249, '선반': 250, '선생님': 251, '성동구': 252, '성북구': 253, '성폭행': 254, '소방관': 255, '소방차': 256, '소화기': 257, '소화전': 258, '손': 259, '손가락': 260, '손목': 261, '송파구': 262, '수영장': 263, '수요일': 264, '술취한 사람': 265, '숨을안쉬다': 266, '시청': 267, '신고하세요(경찰)': 268, '심장마비': 269, '쓰러지다': 270, '아기': 271, '아내': 272, '아들': 273, '아래': 274, '아빠': 275, '아이들': 276, '아저씨': 277, '아줌마': 278, '아파트': 279, '안방': 280, '알려주세요': 281, '앞': 282, '앞집': 283, '약국': 284, '약사': 285, '양천구': 286, '어깨': 287, '어린이': 288, '어제': 289, '어지러움': 290, '언니': 291, '얼굴': 292, '엄마': 293, '엘리베이터': 294, '여자친구': 295, '여학생': 296, '연기': 297, '연락해주세요': 298, '열': 299, '열나다': 300, '열어주세요': 301, '엽총': 302, '영등포구': 303, '옆집': 304, '옆집 아저씨': 305, '옆집 할아버지': 306, '옆집사람': 307, '옆쪽': 308, '오늘': 309, '오른쪽': 310, '오른쪽-귀': 311, '오른쪽-눈': 312, '오빠': 313, '옥상': 314, '올해': 315, '왼쪽': 316, '왼쪽-귀': 317, '왼쪽-눈': 318, '욕실': 319, '용산구': 320, '우리집': 321, '운동장': 322, '월요일': 323, '위': 324, '위에': 325, '위협': 326, '윗집': 327, '윗집사람': 328, '유리': 329, '유치원': 330, '유치원 버스': 331, '은평구': 332, '음식물': 333, '응급대원': 334, '응급처리': 335, '의사': 336, '이마': 337, '이물질': 338, '이번': 339, '이상한사람': 340, '이웃집': 341, '인대': 342, '일요일': 343, '임산부': 344, '임신한아내': 345, '자동차': 346, '자살': 347, '자상': 348, '작년': 349, '작은방': 350, '장난감': 351, '장단지': 352, '절단': 353, '절도': 354, '제초제': 355, '조난': 356, '종로구': 357, '주': 358, '중구': 359, '중랑구': 360, '지난': 361, '지혈대': 362, '진통제': 363, '질식': 364, '집': 365, '집단폭행': 366, '차밖': 367, '차안': 368, '창문': 369, '창백하다': 370, '체온계': 371, '총': 372, '추락': 373, '축사': 374, '출산': 375, '출혈': 376, '친구': 377, '침수': 378, '칼': 379, '코': 380, '탈골': 381, '택시': 382, '토요일': 383, '토하다': 384, '통학버스': 385, '트랙터': 386, '트럭': 387, '파도': 388, '파편': 389, '팔': 390, '팔꿈치': 391, '폭발': 392, '폭우': 393, '폭탄': 394, '폭행': 395, '피나다': 396, '학교': 397, '학생': 398, '할머니': 399, '할아버지': 400, '함몰되다': 401, '해(연)': 402, '해독제': 403, '해열제': 404, '허리': 405, '허벅지': 406, '현관': 407, '현관앞': 408, '협박': 409, '형': 410, '호흡곤란': 411, '호흡기': 412, '홍수': 413, '화상': 414, '화약': 415, '화요일': 416, '화장실': 417, '화재': 418}\n"
     ]
    }
   ],
   "source": [
    "### label - idx mapping정보 가져오기\n",
    "import pickle\n",
    "with open('../data/label_to_idx.pickle', 'rb') as f:\n",
    "    label_to_idx = pickle.load(f)\n",
    "print(label_to_idx)\n",
    "idx_to_label = {value : key for key, value in label_to_idx.items()} ## idx로 label접근\n",
    "\n",
    "\n",
    "num_angles = 30\n",
    "num_classes = len(label_to_idx)\n",
    "frame = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontpath = \"AppleGothic.ttf\"\n",
    "font = ImageFont.truetype(fontpath, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 15:18:39.986 Python[49507:5988198] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742797121.519950 5988198 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Max\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "/Users/hyeonji/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "W0000 00:00:1742797121.594245 5988533 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.605578 5988542 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.607484 5988532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.607788 5988534 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.608187 5988533 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.612386 5988533 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.613665 5988534 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.613704 5988532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742797121.621467 5988541 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-03-24 15:18:42.048 Python[49507:5988198] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-24 15:18:42.048 Python[49507:5988198] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "/var/folders/90/lxvnqq6s6rdbdmylsg5jkpf80000gn/T/ipykernel_49507/2300197019.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  output = loaded_model(torch.tensor(sequence, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  39 : 4\n",
      "acc  tensor(28.8109, grad_fn=<SelectBackward0>)\n",
      "prediction  312 : 오른쪽-눈\n",
      "acc  tensor(25.1522, grad_fn=<SelectBackward0>)\n",
      "prediction  59 : 58\n",
      "acc  tensor(27.1243, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(27.8100, grad_fn=<SelectBackward0>)\n",
      "prediction  59 : 58\n",
      "acc  tensor(29.9776, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(33.2931, grad_fn=<SelectBackward0>)\n",
      "prediction  59 : 58\n",
      "acc  tensor(35.3541, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(34.2285, grad_fn=<SelectBackward0>)\n",
      "prediction  59 : 58\n",
      "acc  tensor(32.7217, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(31.0884, grad_fn=<SelectBackward0>)\n",
      "prediction  59 : 58\n",
      "acc  tensor(30.7789, grad_fn=<SelectBackward0>)\n",
      "prediction  389 : 파편\n",
      "acc  tensor(25.2525, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(37.1683, grad_fn=<SelectBackward0>)\n",
      "prediction  305 : 옆집 아저씨\n",
      "acc  tensor(33.5238, grad_fn=<SelectBackward0>)\n",
      "prediction  277 : 아저씨\n",
      "acc  tensor(32.1701, grad_fn=<SelectBackward0>)\n",
      "prediction  305 : 옆집 아저씨\n",
      "acc  tensor(31.1162, grad_fn=<SelectBackward0>)\n",
      "prediction  278 : 아줌마\n",
      "acc  tensor(26.2478, grad_fn=<SelectBackward0>)\n",
      "prediction  389 : 파편\n",
      "acc  tensor(27.5970, grad_fn=<SelectBackward0>)\n",
      "prediction  351 : 장난감\n",
      "acc  tensor(26.3402, grad_fn=<SelectBackward0>)\n",
      "prediction  389 : 파편\n",
      "acc  tensor(27.1219, grad_fn=<SelectBackward0>)\n",
      "prediction  351 : 장난감\n",
      "acc  tensor(26.1164, grad_fn=<SelectBackward0>)\n",
      "prediction  389 : 파편\n",
      "acc  tensor(26.4445, grad_fn=<SelectBackward0>)\n",
      "prediction  351 : 장난감\n",
      "acc  tensor(25.1412, grad_fn=<SelectBackward0>)\n",
      "prediction  389 : 파편\n",
      "acc  tensor(26.5300, grad_fn=<SelectBackward0>)\n",
      "prediction  273 : 아들\n",
      "acc  tensor(25.3878, grad_fn=<SelectBackward0>)\n",
      "prediction  306 : 옆집 할아버지\n",
      "acc  tensor(25.2887, grad_fn=<SelectBackward0>)\n",
      "prediction  160 : 낯선남자\n",
      "acc  tensor(25.3394, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 비디오\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 웹캠 프레임 크기를 정사각형으로 설정\n",
    "frame_size = 640\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_size)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_size)\n",
    "\n",
    "\n",
    "# holistic설정\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_draw_styles = mp.solutions.drawing_styles\n",
    "\n",
    "### 모델 가져오기 ###\n",
    "graph_args = {\"layout\": \"mediapipe\", \"strategy\": \"spatial\"}\n",
    "loaded_model = Transformer(num_angles=num_angles, num_classes=num_classes)\n",
    "loaded_model.load_state_dict(torch.load(\"../model/transformer2_94_60fps.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "angles_sequence = []\n",
    "\n",
    "\n",
    "sentence = [\" \", ]\n",
    "word = \"\"\n",
    "\n",
    "# 이미지 입력 캡처 및 처리\n",
    "# media pipe 는 RGB\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(imageRGB)\n",
    "\n",
    "    # print(\"왼손 랜드마크: \", results.left_hand_landmarks)\n",
    "    # print(\"오른손 랜드마크: \", results.right_hand_landmarks)\n",
    "    # print(\"얼굴 랜드마크: \", results.face_landmarks)\n",
    "    # print(\"pose 랜드마크: \", results.pose_landmarks)\n",
    "\n",
    "    angles = extract_angle(results)\n",
    "    angles_sequence.append(angles)\n",
    "    sequence = angles_sequence[-frame:]  # 마지막  frame으로 prediction 한다\n",
    "\n",
    "\n",
    "\n",
    "    if len(sequence) == frame:  # 60 프레임\n",
    "\n",
    "        output = loaded_model(torch.tensor(sequence, dtype=torch.float32))\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        prediction_value = prediction.item()\n",
    "\n",
    "        if output[0, prediction_value]>25:\n",
    "            if idx_to_label[prediction_value] != sentence[-1]:\n",
    "                sentence.append(idx_to_label[prediction_value])\n",
    "\n",
    "                print('prediction ', prediction_value, ':', idx_to_label[prediction_value])\n",
    "                print('acc ', output[0, prediction_value]) # (batch, class)\n",
    "\n",
    "                word = f'Prediction: {idx_to_label[prediction_value]} Acc: {output[0, prediction_value].item():.2f}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 점 그리기\n",
    "    annotated_image = image.copy()\n",
    "    mp_draw.draw_landmarks(annotated_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_draw.draw_landmarks(annotated_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "    # 한글 폰트 출력 \n",
    "    img_pil = Image.fromarray(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)) # cv -> PIL\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text((10, 30), f'{word.upper()}', font=font, fill=(255, 255, 255))\n",
    "    img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR) # PIL -> cv\n",
    "\n",
    "    cv2.imshow('output', img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "holistic.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
